{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23e1f3-6288-4829-b417-12bebdeb7e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2467291-da61-46e1-88e4-cf6d26ec59bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f91ff59-f346-4557-bcfe-377d7eca5f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtkinter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtk\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtkinter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filedialog, scrolledtext, ttk\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwhisper\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaLLM\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'whisper'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, ttk\n",
    "from faster_whisper import WhisperModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "import tempfile\n",
    "import uuid\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66190df1-022e-433e-bb60-221e53daca3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OllamaLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Exception during concatenation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m#def concatenate_videos(video_paths, output_path):\u001b[39;00m\n\u001b[32m     69\u001b[39m  \u001b[38;5;66;03m#   ffmpeg_path = 'C:\\\\ffmpeg'\u001b[39;00m\n\u001b[32m     70\u001b[39m   \u001b[38;5;66;03m#  try:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m \n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# ------------------ LLM Setup ------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m llm = \u001b[43mOllamaLLM\u001b[49m(model=\u001b[33m'\u001b[39m\u001b[33mgemma3:1B\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     96\u001b[39m general_template = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[33mYou are Vivi, an expert and friendly video assistant chatbot.\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[33mYou are having an ongoing conversation with the user. You have access to a full transcript of a video. If the user‚Äôs question is about the video, answer helpfully and refer to timestamps if relevant.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33mVivi:\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    113\u001b[39m clipping_template = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[33mYou are an expert video analysis assistant. Given a user query and the transcript of a video with timestamps, identify all timestamp ranges where the video content is relevant to the query. Return the result as a list of timestamp ranges (start and end times in seconds) and a brief explanation of why each range is relevant.\u001b[39m\n\u001b[32m    115\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33m  Relevance: [Brief explanation of why this range is relevant to the query]\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'OllamaLLM' is not defined"
     ]
    }
   ],
   "source": [
    "def _parse_srt_time(t):\n",
    "    try:\n",
    "        h, m, s_ms = t.split(\":\")\n",
    "        if \",\" in s_ms:\n",
    "            s, ms = s_ms.split(\",\")\n",
    "            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "        else:\n",
    "            return int(h) * 3600 + int(m) * 60 + float(s_ms)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        m, s = t.split(\":\")\n",
    "        return int(m) * 60 + float(s)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return float(t)\n",
    "    except:\n",
    "        raise ValueError(f\"Unrecognized timestamp format: {t}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "# ------------------ FFmpeg Functions ------------------\n",
    "\n",
    "def concatenate_videos(video_paths, output_filepath):\n",
    "    try:\n",
    "        if not video_paths:\n",
    "            return \"‚ùå Error: No video clips provided.\", None\n",
    "\n",
    "        ffmpeg_path = \"C:\\\\ffmpeg\\\\ffmpeg.exe\"  \n",
    "\n",
    "        # Create unique list file in temp dir\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        list_file_path = os.path.join(temp_dir, f\"concat_list_{uuid.uuid4().hex[:8]}.txt\")\n",
    "\n",
    "        # Write the list of files to concatenate\n",
    "        with open(list_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for path in video_paths:\n",
    "                f.write(f\"file '{path.replace('\\\\', '/')}'\\n\")\n",
    "\n",
    "        # Run ffmpeg concat\n",
    "        command = [\n",
    "            ffmpeg_path, \"-y\",\n",
    "            \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "            \"-i\", list_file_path,\n",
    "            \"-c\", \"copy\", output_filepath\n",
    "        ]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        # Cleanup list file\n",
    "        os.remove(list_file_path)\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            return f\"‚ùå FFmpeg error:\\n{result.stderr}\", None\n",
    "\n",
    "        return f\"‚úÖ Concatenated video saved to: {output_filepath}\", output_filepath\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Exception during concatenation: {str(e)}\", None\n",
    "\n",
    "\n",
    "#def concatenate_videos(video_paths, output_path):\n",
    " #   ffmpeg_path = 'C:\\\\ffmpeg'\n",
    "  #  try:\n",
    "   #     if not video_paths:\n",
    "    #        return \"Error: No video clips provided.\", None\n",
    "\n",
    "     #   list_file = \"concat_list.txt\"\n",
    "      #  with open(list_file, \"w\") as f:\n",
    "       #     for path in video_paths:\n",
    "        #        f.write(f\"file '{path}'\\n\")\n",
    "\n",
    "#        command = [\n",
    " #           ffmpeg_path, \"-y\", \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "  #          \"-i\", list_file, \"-c\", \"copy\", output_path\n",
    "   #     ]\n",
    "\n",
    "    #    result = subprocess.run(command, capture_output=True, text=True)\n",
    "     #   os.remove(list_file)\n",
    "      #  if result.returncode != 0:\n",
    "       #     return f\"Error concatenating: {result.stderr}\", None\n",
    "        #return f\"‚úÖ Concatenated video saved to {output_path}\", output_path\n",
    "\n",
    "#    except Exception as e:\n",
    " #       return f\"Error concatenating videos: {str(e)}\", None\n",
    "\n",
    "# ------------------ LLM Setup ------------------\n",
    "llm = OllamaLLM(model='gemma3:1B')\n",
    "\n",
    "general_template = \"\"\"\n",
    "You are Vivi, an expert and friendly video assistant chatbot.\n",
    "You are having an ongoing conversation with the user. You have access to a full transcript of a video. If the user‚Äôs question is about the video, answer helpfully and refer to timestamps if relevant.\n",
    "If the question is general and not related to the video, just respond helpfully like a normal assistant.\n",
    "---\n",
    "Conversation History:\n",
    "{context}\n",
    "---\n",
    "Full Transcript of the Video:\n",
    "{transcript}\n",
    "---\n",
    "User:\n",
    "{question}\n",
    "---\n",
    "Vivi:\n",
    "\"\"\"\n",
    "\n",
    "clipping_template = \"\"\"\n",
    "You are an expert video analysis assistant. Given a user query and the transcript of a video with timestamps, identify all timestamp ranges where the video content is relevant to the query. Return the result as a list of timestamp ranges (start and end times in seconds) and a brief explanation of why each range is relevant.\n",
    "\n",
    "Return timestamps as plain numbers with two decimal places (e.g., 31.00, 45.00) without brackets or other characters. Ensure the format is consistent. Ensure end_time is greater than start_time and both are non-negative.\n",
    "\n",
    "Query: {query}\n",
    "Transcript: {transcript}\n",
    "\n",
    "Return the result in the following format:\n",
    "- Range: start_time - end_time\n",
    "  Relevance: [Brief explanation of why this range is relevant to the query]\n",
    "\"\"\"\n",
    "\n",
    "prompt_general = ChatPromptTemplate.from_template(general_template)\n",
    "prompt_clipping = ChatPromptTemplate.from_template(clipping_template)\n",
    "chain_general = prompt_general | llm\n",
    "chain_clipping = prompt_clipping | llm\n",
    "\n",
    "# ------------------ Vivi GUI Class ------------------\n",
    "class ViviChatbot:\n",
    "    def __init__(self):\n",
    "        self.video_path = \"\"\n",
    "        self.context = \"\"\n",
    "        self.transcript_segments = []\n",
    "        self.full_transcript_text = \"\"\n",
    "        self.cap = None\n",
    "        self.playing = False\n",
    "        self.current_frame = 0\n",
    "        self.seek_scale = None\n",
    "        self.audio_process = None\n",
    "\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Vivi Video Chatbot\")\n",
    "\n",
    "        self.chat_display = scrolledtext.ScrolledText(self.root, wrap=tk.WORD, width=80, height=30, font=(\"Arial\", 12))\n",
    "        self.chat_display.pack(padx=10, pady=10)\n",
    "\n",
    "        self.user_entry = tk.Entry(self.root, font=(\"Arial\", 12))\n",
    "        self.user_entry.pack(fill=tk.X, padx=10, pady=(0, 10))\n",
    "        self.user_entry.bind(\"<Return>\", lambda e: self.send_message())\n",
    "\n",
    "        self.send_btn = tk.Button(self.root, text=\"Send\", font=(\"Arial\", 12), command=self.send_message)\n",
    "        self.send_btn.pack(pady=(0, 10))\n",
    "\n",
    "        btn_frame = tk.Frame(self.root)\n",
    "        btn_frame.pack()\n",
    "\n",
    "        self.browse_btn = tk.Button(btn_frame, text=\"üìÇ Upload Video\", command=self.browse_video)\n",
    "        self.browse_btn.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.preview_btn = tk.Button(btn_frame, text=\"‚ñ∂Ô∏è Preview Video\", command=self.play_video)\n",
    "        self.preview_btn.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.progress_bar = ttk.Progressbar(self.root, orient=\"horizontal\", mode=\"determinate\", variable=self.progress_var)\n",
    "        self.progress_bar.pack(fill=tk.X, padx=10, pady=5)\n",
    "\n",
    "        self.chat_display.insert(tk.END, \" Welcome to the Vivi Video Chatbot! Type 'exit' to quit.\\n For getting your video clipped, enter your query in the format:\\n Video clipping:<query>\")\n",
    "        self.chat_display.yview(tk.END)\n",
    "\n",
    "    def format_time(self, seconds):\n",
    "        h = int(seconds // 3600)\n",
    "        m = int((seconds % 3600) // 60)\n",
    "        s = seconds % 60\n",
    "        return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "    def clip_video(self, start_time, end_time):\n",
    "        try:\n",
    "            # Validate time range\n",
    "            if start_time >= end_time:\n",
    "                self.chat_display.insert(tk.END, f\"\\n‚ö† Invalid clip range: start ({start_time}) >= end ({end_time})\\n\")\n",
    "                return \" Invalid time range\", None\n",
    "            \n",
    "            # Use a unique filename to avoid collisions\n",
    "            clip_filename = f\"clip_{uuid.uuid4().hex[:8]}.mp4\"\n",
    "            output_path = os.path.join(tempfile.gettempdir(), clip_filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-y\",\n",
    "                \"-ss\", str(start_time),\n",
    "                \"-to\", str(end_time),\n",
    "                \"-i\", self.video_path,\n",
    "                \"-c\", \"copy\",\n",
    "                output_path\n",
    "            ], check=True)\n",
    "\n",
    "            self.chat_display.yview(tk.END)\n",
    "            return f\"üé¨ Video clip saved to {output_path}\", output_path\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå FFmpeg failed: {e}\\n\")\n",
    "        except PermissionError as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå Permission denied: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            self.chat_display.insert(tk.END, f\"\\n‚ùå Error clipping video: {e}\\n\")\n",
    "    \n",
    "    def transcribe_video(self, video_path):\n",
    "        base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        dir_ = os.path.dirname(video_path)\n",
    "        txt_path = os.path.join(dir_, f\"{base}.txt\")\n",
    "        srt_path = os.path.join(dir_, f\"{base}.srt\")\n",
    "\n",
    "        if os.path.exists(txt_path) and os.path.exists(srt_path):\n",
    "            self.chat_display.insert(tk.END, \"‚úÖ Transcript and subtitles found.\\n\")\n",
    "            with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                blocks = f.read().strip().split(\"\\n\\n\")\n",
    "            segments = []\n",
    "            for block in blocks:\n",
    "                lines = block.split(\"\\n\")\n",
    "                if len(lines) >= 3:\n",
    "                    times = lines[1].split(\" --> \")\n",
    "                    start = _parse_srt_time(times[0])\n",
    "                    end = _parse_srt_time(times[1])\n",
    "                    text = \" \".join(lines[2:])\n",
    "                    segments.append({\"start\": start, \"end\": end, \"text\": text})\n",
    "            return \"\\n\".join(s['text'] for s in segments), segments\n",
    "\n",
    "        self.chat_display.insert(tk.END, \"üîç Running Whisper transcription...\\n\")\n",
    "        model = WhisperModel(\"medium\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        segments_iter, info = model.transcribe(video_path, beam_size=5)\n",
    "        segments = []\n",
    "        for seg in segments_iter:\n",
    "            segments.append({\"start\": seg.start, \"end\": seg.end, \"text\": seg.text})\n",
    "\n",
    "        with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, seg in enumerate(segments):\n",
    "                f.write(f\"{i+1}\\n{format_time(seg['start'])} --> {format_time(seg['end'])}\\n{seg['text']}\\n\\n\")\n",
    "\n",
    "        return \"\\n\".join(s['text'] for s in segments), segments\n",
    "\n",
    "    def send_message(self):\n",
    "        user_input = self.user_entry.get()\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            self.root.destroy()\n",
    "            return\n",
    "\n",
    "        self.chat_display.insert(tk.END, f\"You: {user_input}\\n\")\n",
    "        self.user_entry.delete(0, tk.END)\n",
    "        self.user_entry.config(state=\"disabled\")\n",
    "        self.send_btn.config(state=\"disabled\")\n",
    "\n",
    "        def run_bot():\n",
    "            if user_input.lower().startswith(\"video clipping:\"):\n",
    "                query = user_input[len(\"video clipping:\"):].strip()\n",
    "                response = chain_clipping.invoke({\"query\": query, \"transcript\": self.transcript_segments})\n",
    "                self.chat_display.insert(tk.END, f\"\\nVivi:\")\n",
    "                video_clips = []\n",
    "                for line in response.strip().split(\"\\n\"):\n",
    "                    if line.startswith(\"- Range: \"):\n",
    "                        parts = line[len(\"- Range: \"):].split(\" - \")\n",
    "                        if len(parts) == 2:\n",
    "                            try:\n",
    "                                start = float(parts[0])\n",
    "                                end = float(parts[1])\n",
    "                            except ValueError:\n",
    "                                try:\n",
    "                                    start = _parse_srt_time(parts[0])\n",
    "                                    end = _parse_srt_time(parts[1])\n",
    "                                except Exception as e:\n",
    "                                    self.chat_display.insert(tk.END, f\"\\n‚ö†Ô∏è Failed to parse timestamps: {parts}\")\n",
    "                                    continue\n",
    "                            msg, clip_path = self.clip_video(start, end)\n",
    "                            self.chat_display.insert(tk.END, f\"\\n{msg}\")\n",
    "                            if clip_path:\n",
    "                                video_clips.append(clip_path)\n",
    "                if video_clips:\n",
    "                    msg, out = concatenate_videos(video_clips, output_filepath=os.path.join(os.path.dirname(self.video_path), \"final_output.mp4\"))\n",
    "                    self.chat_display.insert(tk.END, f\"\\n{msg}\\n\")\n",
    "            else:\n",
    "                response = chain_general.invoke({\n",
    "                    \"context\": self.context,\n",
    "                    \"question\": user_input,\n",
    "                    \"transcript\": self.transcript_segments\n",
    "                })\n",
    "                self.chat_display.insert(tk.END, \"Vivi: \")\n",
    "                for word in response.split():\n",
    "                    self.chat_display.insert(tk.END, word + \" \")\n",
    "                    self.chat_display.yview(tk.END)\n",
    "                    time.sleep(0.04)\n",
    "                self.chat_display.insert(tk.END, \"\\n\\n\")\n",
    "                self.context += f\"\\nUser: {user_input}\\nAI: {response}\\n\"\n",
    "            self.user_entry.config(state=\"normal\")\n",
    "            self.send_btn.config(state=\"normal\")\n",
    "            self.user_entry.focus()\n",
    "\n",
    "        threading.Thread(target=run_bot).start()\n",
    "\n",
    "    def browse_video(self):\n",
    "        self.video_path = filedialog.askopenfilename(filetypes=[(\"Video Files\", \"*.mp4 *.mov *.avi\")])\n",
    "        if not self.video_path:\n",
    "            return\n",
    "\n",
    "        self.chat_display.insert(tk.END, f\"\\nüìÅ Selected video: {os.path.basename(self.video_path)}\\n\")\n",
    "\n",
    "        def process_video():\n",
    "            self.full_transcript_text, self.transcript_segments = self.transcribe_video(self.video_path)\n",
    "            self.chat_display.insert(tk.END, \"‚úÖ Transcription completed!\\n\\n\")\n",
    "            self.progress_var.set(0)\n",
    "\n",
    "        threading.Thread(target=process_video).start()\n",
    "\n",
    "    def play_video(self):\n",
    "        if not self.video_path:\n",
    "            self.chat_display.insert(tk.END, \"\\n‚ö†Ô∏è No video loaded yet. Upload a video first.\\n\")\n",
    "            return\n",
    "\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        if not cap.isOpened():\n",
    "            self.chat_display.insert(tk.END, \"\\n‚ùå Failed to open video.\\n\")\n",
    "            return\n",
    "\n",
    "        self.cap = cap\n",
    "        self.current_frame = 0\n",
    "        self.playing = True\n",
    "\n",
    "        # Kill previous audio if running\n",
    "        if self.audio_process and self.audio_process.poll() is None:\n",
    "            self.audio_process.terminate()\n",
    "\n",
    "        # Play audio using ffplay\n",
    "        self.audio_process = subprocess.Popen([\n",
    "            \"ffplay\", \"-nodisp\", \"-autoexit\", \"-loglevel\", \"quiet\", self.video_path\n",
    "        ])\n",
    "\n",
    "        self.video_frame = tk.Toplevel(self.root)\n",
    "        self.video_frame.title(\"üé• Video Player\")\n",
    "\n",
    "        self.canvas = tk.Canvas(self.video_frame, width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                                height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        self.canvas.pack()\n",
    "\n",
    "        control_frame = tk.Frame(self.video_frame)\n",
    "        control_frame.pack()\n",
    "        play_btn = tk.Button(control_frame, text=\"‚ñ∂Ô∏è\", command=lambda: self._play_frames)\n",
    "        play_btn.pack(side=\"left\", padx=5)\n",
    "        pause_btn = tk.Button(control_frame, text=\"‚è∏Ô∏è Pause\", command=self.pause_video)\n",
    "        pause_btn.pack(side=\"left\", padx=5)\n",
    "\n",
    "        self.seek_scale = ttk.Scale(\n",
    "            control_frame,\n",
    "            from_=0,\n",
    "            to=self.cap.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "            orient=\"horizontal\",\n",
    "            length=400,\n",
    "            command=self.seek_video\n",
    "        )\n",
    "        self.seek_scale.pack(side=\"left\", padx=10)\n",
    "\n",
    "        self.playing = True\n",
    "        self._play_frames()\n",
    "\n",
    "    def _play_frames(self):\n",
    "        if not self.playing or self.cap is None:\n",
    "            return\n",
    "\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame)\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.cap.release()\n",
    "            if self.audio_process and self.audio_process.poll() is None:\n",
    "                self.audio_process.terminate()\n",
    "            return\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = ImageTk.PhotoImage(Image.fromarray(frame))\n",
    "        self.canvas.img = img\n",
    "        self.canvas.create_image(0, 0, anchor=\"nw\", image=img)\n",
    "\n",
    "        self.current_frame += 1\n",
    "        self.seek_scale.set(self.current_frame)\n",
    "\n",
    "        delay = int(1000 / self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        self.canvas.after(delay, self._play_frames)\n",
    "\n",
    "    def pause_video(self):\n",
    "        self.playing = False\n",
    "        if self.audio_process and self.audio_process.poll() is None:\n",
    "            self.audio_process.terminate()\n",
    "\n",
    "    def resume_video(self):\n",
    "        if not self.playing:\n",
    "            self.playing = True\n",
    "            self._play_frames()\n",
    "\n",
    "    def seek_video(self, value):\n",
    "        self.current_frame = int(float(value))\n",
    "        if not self.playing:\n",
    "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame)\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                img = ImageTk.PhotoImage(Image.fromarray(frame))\n",
    "                self.canvas.img = img\n",
    "                self.canvas.create_image(0, 0, anchor=\"nw\", image=img)\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "# Start the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    app = ViviChatbot()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff5156-b951-4741-8221-93210594c746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
