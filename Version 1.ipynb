{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0a243a-50e1-4d65-ae7f-b5f64549e868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import whisper\n",
    "import re\n",
    "import requests\n",
    "from pytubefix import YouTube\n",
    "from moviepy import VideoFileClip\n",
    "import subprocess\n",
    "import argparse\n",
    "#from pydub import AudioSegment\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "from moviepy import VideoFileClip\n",
    "import shutil\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de8abaa-1b07-4037-85fb-3e6347d9f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video_from_youtube(url, output_path=\"input_video.mp4\"):\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        stream = yt.streams.filter(progressive=True, file_extension='mp4').get_highest_resolution()\n",
    "        stream.download(filename=output_path)\n",
    "        return output_path\n",
    "    except RuntimeError:\n",
    "        raise FileNotFoundError(\"File does not exist.\")\n",
    "\n",
    "def download_video_from_url(url, output_path=\"input_video.mp4\"):\n",
    "    if \"youtube.com\" in url or \"youtu.be\" in url:\n",
    "        return download_video_from_youtube(url, output_path)\n",
    "    \n",
    "    elif \"drive.google.com\" in url:\n",
    "        try:\n",
    "            file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "        except IndexError:\n",
    "            raise ValueError(\"Google Drive link format incorrect.\")\n",
    "        d_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "        r = requests.get(d_url, stream=True)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return output_path\n",
    "\n",
    "    else: \n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code != 200:\n",
    "            raise ValueError(\"Could not download video from direct link.\")\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return output_path\n",
    "\n",
    "def convert_to_mp4(input_path, output_path=\"input_video.mp4\"):\n",
    "    try:\n",
    "        clip = VideoFileClip(input_path)\n",
    "        clip.write_videofile(output_path, codec='libx264')\n",
    "        clip.close()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Video conversion failed: {e}\")\n",
    "\n",
    "def get_video_input(output_path=\"input_video.mp4\"):\n",
    "    print(\"Choose input method:\")\n",
    "    print(\"1. Upload local video file\")\n",
    "    print(\"2. Provide video URL (YouTube / Drive / MP4)\")\n",
    "    choice = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        file_path = input(\"Enter full path to your local video file: \").strip()\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(\"File does not exist.\")\n",
    "        print(\"Converting to MP4...\")\n",
    "        convert_to_mp4(file_path, output_path)\n",
    "        print(f\"Converted and saved to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        url = input(\"Enter video URL: \").strip()\n",
    "        return download_video_from_url(url)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid choice. Please enter 1 or 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac7d080-edb8-497f-9569-3de520ce565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose input method:\n",
      "1. Upload local video file\n",
      "2. Provide video URL (YouTube / Drive / MP4)\n",
      "Converting to MP4...\n",
      "MoviePy - Building video input_video.mp4.\n",
      "MoviePy - Writing audio in input_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video input_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  99%|█████████▉| 3407/3433 [00:05<00:00, 616.11it/s, now=None]/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:190: UserWarning: In file /Users/mohalsahai/Desktop/Video Clipping/Max Life Videos/Video 1.mp4, 1221120 bytes wanted but 0 bytes read at frame index 3431 (out of a total 3433 frames), at time 142.96/143.06 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:190: UserWarning: In file /Users/mohalsahai/Desktop/Video Clipping/Max Life Videos/Video 1.mp4, 1221120 bytes wanted but 0 bytes read at frame index 3432 (out of a total 3433 frames), at time 143.00/143.06 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready input_video.mp4\n",
      "Converted and saved to: input_video.mp4\n"
     ]
    }
   ],
   "source": [
    "video=get_video_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a5ba6d-6500-4b03-a4b2-8dfee5cae8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_path = '/Users/mohalsahai/Desktop/Video Clipping/Python Packages/ffmpeg'\n",
    "\n",
    "def extract_audio(video_path, audio_path=\"audio.wav\"):\n",
    "    print(f\"Extracting audio from: {video_path}\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            ffmpeg_path, \"-y\", \"-i\", video_path, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", audio_path\n",
    "        ], check=True)\n",
    "        print(f\"Audio saved to: {audio_path}\")\n",
    "        return audio_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise ValueError(f\"Audio extraction failed: {e}\")\n",
    "\n",
    "def extract_subtitles(video_path, subtitle_path=\"subtitles.srt\"):\n",
    "    print(\"Trying to extract subtitles...\")\n",
    "    result = subprocess.run(\n",
    "        [ffmpeg_path, \"-y\", \"-i\", video_path, \"-map\", \"0:s:0\", subtitle_path],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    if \"Stream mapping:\" in result.stderr and os.path.exists(subtitle_path):\n",
    "        print(f\"Subtitles saved to: {subtitle_path}\")\n",
    "        return subtitle_path\n",
    "    else:\n",
    "        print(\"No subtitles found in the video.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3b6c66-c176-4a76-8b79-fdbfb9e63e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from: input_video.mp4\n",
      "Audio saved to: audio.wav\n",
      "Trying to extract subtitles...\n",
      "No subtitles found in the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version N-119686-gae0f71a387-tessus  https://evermeet.cx/ffmpeg/  Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
      "  configuration: --cc=/usr/bin/clang --prefix=/opt/ffmpeg --extra-version=tessus --enable-avisynth --enable-fontconfig --enable-gpl --enable-libaom --enable-libass --enable-libbluray --enable-libdav1d --enable-libfreetype --enable-libgsm --enable-libharfbuzz --enable-libmodplug --enable-libmp3lame --enable-libmysofa --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvmaf --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-version3 --pkg-config-flags=--static --disable-ffplay\n",
      "  libavutil      60.  3.100 / 60.  3.100\n",
      "  libavcodec     62.  3.101 / 62.  3.101\n",
      "  libavformat    62.  0.102 / 62.  0.102\n",
      "  libavdevice    62.  0.100 / 62.  0.100\n",
      "  libavfilter    11.  0.100 / 11.  0.100\n",
      "  libswscale      9.  0.100 /  9.  0.100\n",
      "  libswresample   6.  0.100 /  6.  0.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input_video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.7.100\n",
      "  Duration: 00:02:23.07, start: 0.000000, bitrate: 339 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 848x480, 204 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc61.19.100 libx264\n",
      "  Stream #0:1[0x2](und): Audio: mp3 (mp3float) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    ISFT            : Lavf62.0.102\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, stereo, s16, 512 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc62.3.101 pcm_s16le\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/wav @ 0x7f7bf5812e40] video:0KiB audio:8942KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000852%\n",
      "size=    8942KiB time=00:02:23.07 bitrate= 512.0kbits/s speed=1.16e+03x elapsed=0:00:00.12    \n"
     ]
    }
   ],
   "source": [
    "audio=extract_audio(video)\n",
    "subtitles=extract_subtitles(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea86d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace this with your actual path to ffmpeg\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/path/to/ffmpeg_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4182cdae-ef81-4b52-bd5a-d22ea2939c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:10.000]  Have you ever wondered how to effectively qualify leads in bank assurance?\n",
      "[00:10.000 --> 00:13.000]  Let's explore some real-life scenarios together.\n",
      "[00:16.000 --> 00:19.000]  First, consider the importance of building rapport.\n",
      "[00:19.000 --> 00:28.000]  According to a study by HubSpot, 70% of buyers say that they feel more connected to a salesperson who listens to their needs.\n",
      "[00:31.000 --> 00:35.000]  You can qualify leads list using NOPP criteria.\n",
      "[00:35.000 --> 00:37.000]  N is need for insurance.\n",
      "[00:37.000 --> 00:44.000]  The customer has the need for insurance for child education, protection, retirement, savings.\n",
      "[00:44.000 --> 00:46.000]  O is opportunity to meet.\n",
      "[00:46.000 --> 00:51.000]  There is an opportunity to meet with the customer and get undivided attention for 30 minutes.\n",
      "[00:51.000 --> 00:53.000]  P is physically fit.\n",
      "[00:53.000 --> 00:59.000]  Evaluate the physical fitness of the customer so that correct insurance product can be offered to him her.\n",
      "[00:59.000 --> 01:02.000]  Last P is paying capacity.\n",
      "[01:02.000 --> 01:06.000]  Whether the customer is capable of paying insurance premium.\n",
      "[01:09.000 --> 01:13.000]  For instance, imagine a client walks into a bank branch.\n",
      "[01:13.000 --> 01:19.000]  Instead of jumping straight into product details, ask open-ended questions about their financial goals.\n",
      "[01:19.000 --> 01:23.000]  Questions like, do you have dependents who rely on your income?\n",
      "[01:23.000 --> 01:27.000]  Have you planned for financial security in case of uncertainties?\n",
      "[01:27.000 --> 01:31.000]  Have you considered tax-saving options with additional life cover benefits?\n",
      "[01:31.000 --> 01:35.000]  This approach helps in gathering valuable insights.\n",
      "[01:38.000 --> 01:42.000]  Next, let's talk about qualifying leads effectively.\n",
      "[01:42.000 --> 01:49.000]  A report from McKinsey suggests that qualified leads convert at a rate of 30% higher than unqualified ones.\n",
      "[01:52.000 --> 01:58.000]  Take the case of a financial advisor who uses a checklist to assess client needs.\n",
      "[01:58.000 --> 02:04.000]  By categorizing leads based on their financial situations, they can tailor their offerings more effectively.\n",
      "[02:04.000 --> 02:11.000]  In summary, effective lead handling and qualification can significantly impact your success in bank assurance.\n",
      "[02:11.000 --> 02:14.000]  It's all about understanding your clients and their needs.\n",
      "[02:14.000 --> 02:17.000]  Ready to enhance your lead management skills?\n",
      "Detected language: en\n",
      "Full transcription:\n",
      "  Have you ever wondered how to effectively qualify leads in bank assurance? Let's explore some real-life scenarios together. First, consider the importance of building rapport. According to a study by HubSpot, 70% of buyers say that they feel more connected to a salesperson who listens to their needs. You can qualify leads list using NOPP criteria. N is need for insurance. The customer has the need for insurance for child education, protection, retirement, savings. O is opportunity to meet. There is an opportunity to meet with the customer and get undivided attention for 30 minutes. P is physically fit. Evaluate the physical fitness of the customer so that correct insurance product can be offered to him her. Last P is paying capacity. Whether the customer is capable of paying insurance premium. For instance, imagine a client walks into a bank branch. Instead of jumping straight into product details, ask open-ended questions about their financial goals. Questions like, do you have dependents who rely on your income? Have you planned for financial security in case of uncertainties? Have you considered tax-saving options with additional life cover benefits? This approach helps in gathering valuable insights. Next, let's talk about qualifying leads effectively. A report from McKinsey suggests that qualified leads convert at a rate of 30% higher than unqualified ones. Take the case of a financial advisor who uses a checklist to assess client needs. By categorizing leads based on their financial situations, they can tailor their offerings more effectively. In summary, effective lead handling and qualification can significantly impact your success in bank assurance. It's all about understanding your clients and their needs. Ready to enhance your lead management skills?\n",
      "[0.00 - 10.00]  Have you ever wondered how to effectively qualify leads in bank assurance?\n",
      "[10.00 - 13.00]  Let's explore some real-life scenarios together.\n",
      "[16.00 - 19.00]  First, consider the importance of building rapport.\n",
      "[19.00 - 28.00]  According to a study by HubSpot, 70% of buyers say that they feel more connected to a salesperson who listens to their needs.\n",
      "[31.00 - 35.00]  You can qualify leads list using NOPP criteria.\n",
      "[35.00 - 37.00]  N is need for insurance.\n",
      "[37.00 - 44.00]  The customer has the need for insurance for child education, protection, retirement, savings.\n",
      "[44.00 - 46.00]  O is opportunity to meet.\n",
      "[46.00 - 51.00]  There is an opportunity to meet with the customer and get undivided attention for 30 minutes.\n",
      "[51.00 - 53.00]  P is physically fit.\n",
      "[53.00 - 59.00]  Evaluate the physical fitness of the customer so that correct insurance product can be offered to him her.\n",
      "[59.00 - 62.00]  Last P is paying capacity.\n",
      "[62.00 - 66.00]  Whether the customer is capable of paying insurance premium.\n",
      "[69.00 - 73.00]  For instance, imagine a client walks into a bank branch.\n",
      "[73.00 - 79.00]  Instead of jumping straight into product details, ask open-ended questions about their financial goals.\n",
      "[79.00 - 83.00]  Questions like, do you have dependents who rely on your income?\n",
      "[83.00 - 87.00]  Have you planned for financial security in case of uncertainties?\n",
      "[87.00 - 91.00]  Have you considered tax-saving options with additional life cover benefits?\n",
      "[91.00 - 95.00]  This approach helps in gathering valuable insights.\n",
      "[98.00 - 102.00]  Next, let's talk about qualifying leads effectively.\n",
      "[102.00 - 109.00]  A report from McKinsey suggests that qualified leads convert at a rate of 30% higher than unqualified ones.\n",
      "[112.00 - 118.00]  Take the case of a financial advisor who uses a checklist to assess client needs.\n",
      "[118.00 - 124.00]  By categorizing leads based on their financial situations, they can tailor their offerings more effectively.\n",
      "[124.00 - 131.00]  In summary, effective lead handling and qualification can significantly impact your success in bank assurance.\n",
      "[131.00 - 134.00]  It's all about understanding your clients and their needs.\n",
      "[134.00 - 137.00]  Ready to enhance your lead management skills?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import subprocess\n",
    "\n",
    "# --- Custom FFmpeg Path ---\n",
    "ffmpeg_path = '/Users/mohalsahai/Desktop/Video Clipping/Python Packages/ffmpeg'\n",
    "\n",
    "# --- Add to PATH so whisper can find it ---\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.dirname(ffmpeg_path)\n",
    "\n",
    "# --- Whisper Transcription ---\n",
    "model = whisper.load_model(\"medium\")\n",
    "result = model.transcribe(\"audio.wav\", verbose=True, task='transcribe', language='en', fp16=False)\n",
    "\n",
    "# --- Output ---\n",
    "print(\"Detected language:\", result['language'])\n",
    "print(\"Full transcription:\\n\", result['text'])\n",
    "\n",
    "timestamped_transcript=\"\"\n",
    "for segment in result[\"segments\"]:\n",
    "    timestamped_transcript+=f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\\n\"\n",
    "    print(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n",
    "\n",
    "# --- Save Full Transcript ---\n",
    "with open(\"transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"text\"])\n",
    "\n",
    "# --- Save SRT File ---\n",
    "def save_srt(segments, path=\"transcript.srt\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, seg in enumerate(segments):\n",
    "            start = seg['start']\n",
    "            end = seg['end']\n",
    "            text = seg['text']\n",
    "            f.write(f\"{i+1}\\n\")\n",
    "            f.write(f\"{format_time(start)} --> {format_time(end)}\\n\")\n",
    "            f.write(f\"{text.strip()}\\n\\n\")\n",
    "\n",
    "# --- Format Time for SRT ---\n",
    "def format_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02}:{m:02}:{s:06.3f}\".replace(\".\", \",\")\n",
    "\n",
    "save_srt(result['segments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e8ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_similar_transcript_segments(text_query, transcript_segments, top_k=5):\n",
    "    query_embedding = embedder.encode(text_query, convert_to_tensor=True)\n",
    "    texts = [seg[\"text\"] for seg in transcript_segments]\n",
    "    text_embeddings = embedder.encode(texts, convert_to_tensor=True, batch_size=32)\n",
    "\n",
    "    similarities = util.cos_sim(query_embedding, text_embeddings)[0]\n",
    "    ranked = sorted(zip(transcript_segments, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    results = []\n",
    "    for segment, score in ranked[:top_k]:\n",
    "        results.append({\n",
    "            \"text\": segment[\"text\"],\n",
    "            \"start\": segment[\"start\"],\n",
    "            \"end\": segment[\"end\"],\n",
    "            \"similarity\": float(score)\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6cb4",
   "metadata": {},
   "source": [
    "Chatbot with UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d677cd9c-e61d-44de-a055-7d7eb30e549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "llm = OllamaLLM(model='gemma3')\n",
    "\n",
    "# Template for general queries\n",
    "general_template = \"\"\"\n",
    "You are Vivi, an expert and friendly video assistant chatbot.\n",
    "\n",
    "You are having an ongoing conversation with the user. You have access to a full transcript of a video. If the user’s question is about the video, answer helpfully and refer to timestamps if relevant.\n",
    "\n",
    "If the question is general and not related to the video, just respond helpfully like a normal assistant. You can use your general knowledge to help the user even if it’s unrelated to the transcript.\n",
    "\n",
    "---\n",
    "Conversation History:\n",
    "{context}\n",
    "\n",
    "---\n",
    "Full Transcript of the Video:\n",
    "{transcript}\n",
    "\n",
    "---\n",
    "User:\n",
    "{question}\n",
    "\n",
    "---\n",
    "Vivi:\n",
    "\"\"\"\n",
    "\n",
    "# Template for topic extraction\n",
    "topic_template = \"\"\"\n",
    "You are an expert video analysis assistant. Given the transcript of a video with timestamps, identify the major topics discussed and provide their corresponding timestamp ranges. Return the result as a list of topics, each with a brief description and its start and end timestamps.\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Return the result in the following format:\n",
    "- Topic: [Brief description]\n",
    "  Timestamps: [start_time] - [end_time]\n",
    "\"\"\"\n",
    "\n",
    "prompt_general = ChatPromptTemplate.from_template(general_template)\n",
    "prompt_topic = ChatPromptTemplate.from_template(topic_template)\n",
    "chain_general = prompt_general | llm\n",
    "chain_topic = prompt_topic | llm\n",
    "\n",
    "# FFmpeg-based video clipping function\n",
    "def clip_video(video_path, start_time, end_time, output_path=None):\n",
    "    \"\"\"\n",
    "    Clip a video from start_time to end_time using FFmpeg.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        start_time (float): Start time in seconds.\n",
    "        end_time (float): End time in seconds.\n",
    "        output_path (str): Path to save the clipped video. If None, generates a default name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Success message with output path or error message.\n",
    "    \"\"\"\n",
    "    ffmpeg_path = '/Users/mohalsahai/Desktop/Video Clipping/Python Packages/ffmpeg'\n",
    "    try:\n",
    "        if not os.path.exists(video_path):\n",
    "            return f\"Error: Video file {video_path} does not exist.\"\n",
    "        \n",
    "        if output_path is None:\n",
    "            base, ext = os.path.splitext(video_path)\n",
    "            output_path = f\"{base}_clip_{int(start_time)}_{int(end_time)}.mp4\"\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # FFmpeg command: -ss for start, -t for duration, -c copy for stream copying\n",
    "        command = [\n",
    "            ffmpeg_path, \"-y\",  # Overwrite output if exists\n",
    "            \"-i\", video_path,   # Input file\n",
    "            \"-ss\", str(start_time),  # Start time\n",
    "            \"-t\", str(duration),     # Duration\n",
    "            \"-c\", \"copy\",       # Copy streams without re-encoding\n",
    "            output_path         # Output file\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        return f\"Video clip saved to {output_path}\"\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error clipping video: FFmpeg failed with {e.stderr}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error clipping video: {str(e)}\"\n",
    "\n",
    "def start_chat_ui(transcript_segments, full_transcript_text):\n",
    "    context = \"\"\n",
    "    response_queue = queue.Queue()\n",
    "    video_path = video  # Use the video variable from Cell 3\n",
    "\n",
    "    def is_video_clipping_query(query):\n",
    "        return query.lower().startswith(\"video clipping:\")\n",
    "\n",
    "    def extract_query(query):\n",
    "        match = re.match(r\"Video Clipping:\\\\s*(.+)\", query, re.IGNORECASE)\n",
    "        return match.group(1).strip() if match else query\n",
    "\n",
    "    def get_major_topics():\n",
    "        try:\n",
    "            response = chain_topic.invoke({\"transcript\": full_transcript_text})\n",
    "            topics = []\n",
    "            lines = response.split(\"\\n\")\n",
    "            current_topic = None\n",
    "            for line in lines:\n",
    "                if line.startswith(\"- Topic:\"):\n",
    "                    current_topic = {\"description\": line.replace(\"- Topic:\", \"\").strip()}\n",
    "                elif line.startswith(\"  Timestamps:\") and current_topic:\n",
    "                    times = line.replace(\"  Timestamps:\", \"\").strip().split(\" - \")\n",
    "                    if len(times) == 2:\n",
    "                        current_topic[\"start\"] = float(times[0])\n",
    "                        current_topic[\"end\"] = float(times[1])\n",
    "                        topics.append(current_topic)\n",
    "                        current_topic = None\n",
    "            return topics\n",
    "        except Exception as e:\n",
    "            return [{\"description\": f\"Error extracting topics: {str(e)}\", \"start\": 0, \"end\": 0}]\n",
    "\n",
    "    def find_most_similar_topic(query, topics):\n",
    "        if not topics:\n",
    "            return None\n",
    "        query_embedding = embedder.encode(query)\n",
    "        topic_descriptions = [topic[\"description\"] for topic in topics]\n",
    "        topic_embeddings = embedder.encode(topic_descriptions)\n",
    "        similarities = util.cos_sim(query_embedding, topic_embeddings)[0]\n",
    "        max_idx = similarities.argmax()\n",
    "        return topics[max_idx], float(similarities[max_idx])\n",
    "\n",
    "    def send_message():\n",
    "        nonlocal context\n",
    "        user_input = user_entry.get()\n",
    "        if user_input.strip().lower() == \"exit\":\n",
    "            root.destroy()\n",
    "            return\n",
    "\n",
    "        chat_display.insert(tk.END, f\"User: {user_input}\\n\")\n",
    "        user_entry.delete(0, tk.END)\n",
    "\n",
    "        def run_bot():\n",
    "            try:\n",
    "                if is_video_clipping_query(user_input):\n",
    "                    query = extract_query(user_input)\n",
    "                    topics = get_major_topics()\n",
    "                    if not topics or \"Error\" in topics[0][\"description\"]:\n",
    "                        response_queue.put((topics[0][\"description\"], []))\n",
    "                        return\n",
    "\n",
    "                    best_topic, similarity = find_most_similar_topic(query, topics)\n",
    "                    if not best_topic or similarity < 0.1:\n",
    "                        response_queue.put((\"No relevant topic found for the video.\", []))\n",
    "                        return\n",
    "\n",
    "                    result = clip_video(\n",
    "                        video_path,\n",
    "                        best_topic[\"start\"],\n",
    "                        best_topic[\"end\"],\n",
    "                        output_path=f\"clipped_{int(best_topic['start'])}_{int(best_topic['end'])}.mp4\"\n",
    "                    )\n",
    "                    response = f\"{result}\\nTopic: {best_topic['description']}\\nTimestamps: {best_topic['start']:.2f} - {best_topic['end']:.2f}\\nSimilarity: {similarity:.2f}\"\n",
    "                    response_queue.put((response, []))\n",
    "                else:\n",
    "                    top_segments = find_similar_transcript_segments(user_input, transcript_segments, top_k=3)\n",
    "                    response = chain_general.invoke({\n",
    "                        \"context\": context,\n",
    "                        \"question\": user_input,\n",
    "                        \"transcript\": full_transcript_text,\n",
    "                    })\n",
    "                    response_queue.put((response, top_segments))\n",
    "            except Exception as e:\n",
    "                response_queue.put((f\"Error: {str(e)}\", []))\n",
    "\n",
    "        def check_queue():\n",
    "            try:\n",
    "                response, top_segments = response_queue.get_nowait()\n",
    "                chat_display.insert(tk.END, f\"Vivi: {response}\\n\")\n",
    "                if top_segments:\n",
    "                    chat_display.insert(tk.END, \"Relevant segments:\\n\" + \"\\n\".join(\n",
    "                        [f\"[{seg['start']:.2f} - {seg['end']:.2f}]: {seg['text']}\" for seg in top_segments]) + \"\\n\")\n",
    "                chat_display.yview(tk.END)\n",
    "                nonlocal context\n",
    "                context += f\"\\nUser: {user_input}\\nAI: {response}\\n\"\n",
    "            except queue.Empty:\n",
    "                root.after(100, check_queue)\n",
    "\n",
    "        threading.Thread(target=run_bot, daemon=True).start()\n",
    "        root.after(100, check_queue)\n",
    "\n",
    "    def on_closing():\n",
    "        for thread in threading.enumerate()[1:]:\n",
    "            thread.join(timeout=1.0)\n",
    "        root.destroy()\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Vivi Video Chatbot\")\n",
    "    chat_display = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=80, height=30, font=(\"Arial\", 12))\n",
    "    chat_display.pack(padx=10, pady=10)\n",
    "    user_entry = tk.Entry(root, font=(\"Arial\", 12))\n",
    "    user_entry.pack(fill=tk.X, padx=10, pady=(0, 10))\n",
    "    user_entry.bind(\"<Return>\", lambda e: send_message())\n",
    "    send_btn = tk.Button(root, text=\"Send\", font=(\"Arial\", 12), command=send_message)\n",
    "    send_btn.pack(pady=(0, 10))\n",
    "    chat_display.insert(tk.END, \"Welcome to the Vivi Video Chatbot! Type 'exit' to quit.\\nFor video clipping, use 'Video Clipping: {Query}'.\\n\")\n",
    "    chat_display.yview(tk.END)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb02f9de-5b83-46a7-ac6d-f6928c38d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_transcript_with_timestamps(segments):\n",
    "    lines = []\n",
    "    for seg in segments:\n",
    "        start = format_time(seg['start'])\n",
    "        end = format_time(seg['end'])\n",
    "        text = seg['text'].strip()\n",
    "        lines.append(f\"[{start} - {end}] {text}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3304a-c578-4f34-8c2f-d79699c1c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_chat_ui(result[\"segments\"], timestamped_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25618e31-2076-4fa2-8039-29094d6ddf68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timestamped_transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtimestamped_transcript\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'timestamped_transcript' is not defined"
     ]
    }
   ],
   "source": [
    "print(timestamped_transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
